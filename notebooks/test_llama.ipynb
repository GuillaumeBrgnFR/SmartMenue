{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f3d4c84c0c42f6a6428ba084b647a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13791b74fc18402780a6560caca5e562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c0d4c45b644a2881b152e88f59f57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2aa8a7293e94a8bae31879dd9b3643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aab56340de748dabc7ef6c782367830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75731898cc9f42dc8b54de00f0e744af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c6e36038744236a280406381840b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4603bdc4fe814b90ac2f1a1868e53336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/e1/83/e1838a8d2ba17bb61ef1fc8f6819407ea8d672b8e762f49052972249b3b5e224/d43476fdd2fca0c44d55ee930039dd5dafb6331764dc0b5e5f89c60b551fcc12?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00002.safetensors%3B+filename%3D%22model-00001-of-00002.safetensors%22%3B&Expires=1732271083&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjI3MTA4M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lMS84My9lMTgzOGE4ZDJiYTE3YmI2MWVmMWZjOGY2ODE5NDA3ZWE4ZDY3MmI4ZTc2MmY0OTA1Mjk3MjI0OWIzYjVlMjI0L2Q0MzQ3NmZkZDJmY2EwYzQ0ZDU1ZWU5MzAwMzlkZDVkYWZiNjMzMTc2NGRjMGI1ZTVmODljNjBiNTUxZmNjMTI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=gDa8bHJXPvCebfb5V8q%7EmQ3Dsm8nfAItBpFucthYfZneFjfLBBZRn4MAGEYrCvCqKyMLuKhQ1hHHMmR2AiSEoUXNhDvBV7Uve1-f4lq8EpHA%7EmOLVf4ew-G9FUkHb3eCP-5KEdtE1sKrZta0yrtfnlbkZgT7BmOlxnwc7i6m-txIu-UHRpTSJUcOUAIwjuorQFfR69ZCaTh5YTxXh2%7EcSGQHpsZ1lX6Bi6Meg86WBLFHf4%7ELJSfCg407HtPrciVyi07MpWFlPVYalQxvFFSnPYxjYZGNl8D0FkTsf4wLaYPs5Efq7nm8AqQnNY%7E8pG97Oh%7E6u-2axpNKw2il0kwuYQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3091f0f892944fa2985c8ef87fa7bf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  36%|###5      | 3.58G/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1108824f2ce147dd9d9e0d2598223219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/e1/83/e1838a8d2ba17bb61ef1fc8f6819407ea8d672b8e762f49052972249b3b5e224/94712c4b457815e0f8aaa4f78170aebba2b9ee3339ff2b40fceb983338fbafee?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00002.safetensors%3B+filename%3D%22model-00002-of-00002.safetensors%22%3B&Expires=1732271668&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjI3MTY2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lMS84My9lMTgzOGE4ZDJiYTE3YmI2MWVmMWZjOGY2ODE5NDA3ZWE4ZDY3MmI4ZTc2MmY0OTA1Mjk3MjI0OWIzYjVlMjI0Lzk0NzEyYzRiNDU3ODE1ZTBmOGFhYTRmNzgxNzBhZWJiYTJiOWVlMzMzOWZmMmI0MGZjZWI5ODMzMzhmYmFmZWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qb0FW1RTxexTfbNrEknpWM-pH5XmsB8DihI8pXlsonjg3vObgm3M98lJ-SMeUBDjCS8J5h-T6L6ToCrKBmeakfjjZ6mPAMbMHsq0Q6k8IKCkWIT9gcSSSLDYsCgoNMqs3O8tAB6e2k9csXlKE6--RPTQos6GsN50rxBbGgyCCaiGiWmHK5-73KrITLCDeoi4i5%7EzyS9Y0%7EHOjr9Jn7t1lq48ivVduyx74AE76SWCqKlIWE9yaPdjUlWjg81VPtd16qyh8mRzU57RT0kFYPNlOAo0H3jFxapUv%7EKINQJbPGEk6DFMLL0HMokC3Ikm9MG5CxhidtEMQEKWdaEUAVvy2g__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3574aa9d4f44594babf93d06a77d552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:  13%|#2        | 451M/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0abdb1756d4d42a43d7cb63147df2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb69977c2be48b1b4aed32e7ad994a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"huggyllama/llama-7b\"  # Remplace par le nom du modèle si différent\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "ENTRÉE\n",
    "BUFALA E RUCOLA\n",
    "tomates-cerise, mozzarella, huile d'olive extra vierge, basilic & roquette.\n",
    "\n",
    "PLAT PRINCIPAL\n",
    "SAUMON GRILLÉ\n",
    "saumon bio grillé servi avec scarole sautée.\n",
    "\n",
    "TAGLIATA\n",
    "piccata de veau au citron & fleurs de câpres.\n",
    "\n",
    "DESSERT\n",
    "TIRAMISÜ\n",
    "tiramisü classique aux copeaux de chocolat avec crème fouettée.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt pour demander la conversion en JSON\n",
    "prompt = f\"\"\"\n",
    "Voici un menu de restaurant sous forme de texte non structuré :\n",
    "{input_text}\n",
    "\n",
    "Structure le texte en JSON sous la forme suivante :\n",
    "{{\n",
    "  \"entree\": [ ... ],\n",
    "  \"plats principaux\": [ ... ],\n",
    "  \"desserts\": [ ... ]\n",
    "}}\n",
    "en changeant les clés selon les catégories de plats.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voici un menu de restaurant sous forme de texte non structuré :\n",
      "\n",
      "ENTRÉE\n",
      "BUFALA E RUCOLA\n",
      "tomates-cerise, mozzarella, huile d'olive extra vierge, basilic & roquette.\n",
      "\n",
      "PLAT PRINCIPAL\n",
      "SAUMON GRILLÉ\n",
      "saumon bio grillé servi avec scarole sautée.\n",
      "\n",
      "TAGLIATA\n",
      "piccata de veau au citron & fleurs de câpres.\n",
      "\n",
      "DESSERT\n",
      "TIRAMISÜ\n",
      "tiramisü classique aux copeaux de chocolat avec crème fouettée.\n",
      "\n",
      "\n",
      "Structure le texte en JSON sous la forme suivante :\n",
      "{\n",
      "  \"entree\": [ ... ],\n",
      "  \"plats principaux\": [ ... ],\n",
      "  \"desserts\": [ ... ]\n",
      "}\n",
      "en changeant les clés selon les catégories de plats.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Génération de texte\n",
    "result = llama_pipeline(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Résultat généré\n",
    "generated_text = result[0]['generated_text']\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaume/Documents/Hackathon/SmartMenue/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:2869: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0730954789043e8852b764f7ff3d310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./models/llama_model/tokenizer_config.json',\n",
       " './models/llama_model/special_tokens_map.json',\n",
       " './models/llama_model/tokenizer.model',\n",
       " './models/llama_model/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chemin où vous voulez enregistrer le modèle\n",
    "save_directory = \"./models/llama_model\"\n",
    "\n",
    "# Enregistrer le modèle\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Enregistrer le tokenizer\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63c291616834be6aa1f6c6024215e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62734ca484644b27b6c4dada700f8828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bdf857150b40c18273dc65ace0efe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262a73b513e74c17a97108d3a544b709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf52e57a8dd94400a1c79828dd01a149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/9.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a149261b157b479bb5550b36fed1165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/dd/56/dd5620146652f7cccf4f7bb10b6c9f7e69cca55b55fa58ea43f33aa610ea29c2/60a2c0b6f284a55df23ee68ce3065631368d7a6a35e5443b5cd98151f145fd67?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00003-of-00005.safetensors%3B+filename%3D%22model-00003-of-00005.safetensors%22%3B&Expires=1732284794&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjI4NDc5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kZC81Ni9kZDU2MjAxNDY2NTJmN2NjY2Y0ZjdiYjEwYjZjOWY3ZTY5Y2NhNTViNTVmYTU4ZWE0M2YzM2FhNjEwZWEyOWMyLzYwYTJjMGI2ZjI4NGE1NWRmMjNlZTY4Y2UzMDY1NjMxMzY4ZDdhNmEzNWU1NDQzYjVjZDk4MTUxZjE0NWZkNjc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=laavPSNLST6ZqdIWOAu9heVxMx60oIdPucmBzOZzxlENY-byicQ%7EgDeX1Q4uqfTqx0azMUZtQW5ZdNYY3DSrOU1oYd64SAr%7ElHG7AbWZ9D02%7EM8WVgTs1J3eyj9ks9kfVUfIrzT0DXpKOAcZWyQkIZrFvsOT6zPq7Td%7EHnbnafM8%7EXA4i2wzGC%7EpWPvTxgP30uGLPNe9MFn19HsJOI3QLk8OMIHFjPk8kqpJC5H1Qv3t2oVDZAOGBwNryM9xrA0Ty5GwCz92GXcI7--6VUFcVSR-rHQI3UyJ31qPr2az0gN8Pgs2A2XbB8wjQNqqem%7EQndTn8w5qhGNazVF%7E2Z64Iw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cb30fff16a456195a94f447dc533dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:  43%|####2     | 4.25G/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bcfa2d501c4183962afe62d8e0d0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef083775c5d94412a70779e45a001a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/6.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e60f33890c44d998231b75022e0b868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6c2766c9854dcca3a0243a3eade110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853cbebb7e624e2e8d25e3a5193167de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4454dd0f49949a0a61b439a418f9017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034f44e8ecdf437193274ecc221244d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d112296ad2cd4ca5872b476ea5935d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./models/flan_model_xxl/tokenizer_config.json',\n",
       " './models/flan_model_xxl/special_tokens_map.json',\n",
       " './models/flan_model_xxl/spiece.model',\n",
       " './models/flan_model_xxl/added_tokens.json',\n",
       " './models/flan_model_xxl/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "\n",
    "save_directory=\"./models/flan_model_xxl\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A student is learning computer science. He is learning how to use a computer. He is learning how to use a computer to do a project.']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Write a story about a student who is learning computer science\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=500)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"\"\"\"Voici une extraction de texte. Crée moi un dictionnaire ou chaque clé est une section du menu de restaurant et qui contient la liste des plats \"\", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \"De\", \"la\", \"ferme...\", \"\", \"\", \"\", \"Cuisse\", \"de\", \"canard\", \"confit,\", \"\", \"frites\", \"maison,\", \"salade\", \"15.90\", \"\\u20ac\", \"\", \"\", \"\", \"Le\", \"magret\", \"de\", \"canard\", \"servi\", \"entier,\", \"frites\", \"\", \"maison\", \"ou\", \"salade,\", \"sauce\", \"Bordelaise\", \"..22.90\", \"\\u20ac\", \"\", \"\", \"\", \"Sauce\", \"au\", \"choix\", \":\", \"roquefort,\", \"poivre\", \"vert,\", \"\", \"bordelaise,\", \"tartare\", \"\", \"\", \"\", \"A\", \"la\", \"cri\\u00e9e\\u2026.\", \"\", \"\", \"\", \"Poisson\", \"du\", \"moment,\", \"selon\", \"arrivage\\u2026\", \"16.50\", \"\\u20ac\", \"\", \"Accompagn\\u00e9\", \"de\", \"l\\u00e9gumes\", \"et\", \"riz\", \"\", \"\", \"\", \"Menu\", \"Petits\", \"Girondins\", \"\", \"9.90\", \"\\u20ac\", \"\", \"\", \"\", \"Jusqu\\u2019\\u00e0\", \"12\", \"ans\", \"\", \"\", \"\", \"1\", \"soda\", \"au\", \"choix\", \"\", \"Coca\", \"Cola,\", \"limonade,\", \"Ice\", \"Tea\", \"p\\u00eache,\", \"sirop\", \"\", \"\", \"\", \"Steak\", \"hach\\u00e9\", \"ou\", \"Fish\\u2019n\\u2019Chips\", \"\", \"Glace\", \"(chocolat,\", \"vanille,\", \"fraise)\", \"\", \"\", \"\", \"ou\", \"\", \"Moelleux\", \"au\", \"chocolat\", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \"Menu\", \"v\\u00e9g\\u00e9tarien\", \"\", \"19.50\", \"\\u20ac\", \"\", \"\", \"\", \"Tapenade\", \"d\\u2019olives\", \"noires\", \"maison\", \"et\", \"ses\", \"\", \"toasts\", \"\", \"\", \"\", \"Tatin\", \"de\", \"l\\u00e9gumes\", \"au\", \"pesto\", \"\", \"et\", \"salade\", \"fra\\u00eecheur\", \"\", \"(Crudit\\u00e9s)\", \"\", \"\", \"Salade\", \"de\", \"fruits\", \"\", \"\", \"\", \"Ou\", \"\", \"\", \"\", \"Dessert\", \"au\", \"choix\", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \"Nos\", \"Desserts\", \"\", \"\", \"\", \"Niniche\", \"Bordelaise\", \"6.90\", \"\\u20ac\", \"\", \"\", \"\", \"Panna\", \"cotta\", \"(fruits\", \"rouges\", \"ou\", \"caramel)...\", \"6.90\", \"\\u20ac\", \"\", \"\", \"\", \"Tiramisu...\", \"7.50\", \"\\u20ac\", \"\", \"Cr\\u00e8me\", \"br\\u00dcl\\u00e9e.ssseeremvennenunenee\", \"150\\u20ac\", \"\", \"Cr\\u00e8me\", \"Citron\", \"/\", \"Passion\", \"Sp\\u00e9culoos\", \"7.50\", \"\\u20ac\", \"\", \"Coulant\", \"au\", \"chocolat...\", \"7.50\", \"\\u20ac\", \"\", \"Les\", \"trois\", \"boules\", \"de\", \"glace\", \"7.50\", \"\\u20ac\", \"\", \"(Vanille,\", \"chocolat,\", \"fraise)\", \"\", \"\", \"Pain\", \"perdu\", \"et\", \"sa\", \"glace\", \"vanille...\", \"8.50\", \"\\u20ac\", \"\", \"Caf\\u00e9\", \"gourmande\", \"8.50\\u20ac\", \"\", \"\", \"\", \"Brebis\", \"fermier\", \"de\", \"la\", \"vall\\u00e9e\", \"d\\u2019Ossau,\", \"\", \"confiture\", \"de\", \"cerises\", \"noires...\", \"7.90\", \"\\u20ac\", \"\", \"\", \"\", \" \", \"\", \"\", \"\", \"\" Ceci\"\"\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=2000)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donne moi dictionnaire Python structuré avec le nom des catégories de plats comme clés et les plats correspondants comme valeurs.\n"
     ]
    }
   ],
   "source": [
    "# Texte brut extrait par PyTesseract\n",
    "input_text = \"\"\"\n",
    "+\\n\\nENTRÉE\\n\\nBUFALA E RUCOLA\\n\\ntomates-cerise, mozzarella, huile d'olive\\nextra vierge, basilic & roquette.\\n\\nPLAT PRINCIPAL\\n\\nSAUMON GRILLÉ\\n\\nsaumon bio grillé servi avec scarole\\nsautée\\n\\nTAGLIATA\\n\\npiccata de veau au citron & fleurs de\\ncâpres\\n\\nDESSERT\\n\\nTIRAMISÜ\\n\\ntiramisü classique aux copeaux de\\nchocolat avec crème fouettée\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Voici un menu de restaurant extrait sous forme de texte brut :\n",
    "{input_text}\n",
    "\n",
    "Donne moi dictionnaire Python structuré avec le nom des catégories de plats comme clés et les plats correspondants comme valeurs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Encoder l'input long\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "# Générer la sortie\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=1000,  # Ajuster pour la taille de la sortie attendue\n",
    "    num_beams=4,     # Améliorer la précision de génération\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# Décoder et afficher le résultat\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
