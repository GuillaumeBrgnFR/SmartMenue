{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c2ca1dcda2447db1fd24cb92d0501a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d163a65897134255845acedfcfc5d161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38a981462d644c9b390575d69ffb30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48907599fa264bec8ffe7d4873d3ad60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dfb697af214afea0999084a36391c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7406bc04c1943abb02506c6dca3a99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/50.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f02b7cd4c543a8838d34d846bc1ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffd230e15d74504a1be2f721c0159e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00007.safetensors:   0%|          | 0.00/9.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"huggyllama/llama-30b\"  # Remplace par le nom du modèle si différent\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "ENTRÉE\n",
    "BUFALA E RUCOLA\n",
    "tomates-cerise, mozzarella, huile d'olive extra vierge, basilic & roquette.\n",
    "\n",
    "PLAT PRINCIPAL\n",
    "SAUMON GRILLÉ\n",
    "saumon bio grillé servi avec scarole sautée.\n",
    "\n",
    "TAGLIATA\n",
    "piccata de veau au citron & fleurs de câpres.\n",
    "\n",
    "DESSERT\n",
    "TIRAMISÜ\n",
    "tiramisü classique aux copeaux de chocolat avec crème fouettée.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt pour demander la conversion en JSON\n",
    "prompt = f\"\"\"\n",
    "Voici un menu de restaurant sous forme de texte non structuré :\n",
    "{input_text}\n",
    "\n",
    "Structure le texte en JSON sous la forme suivante :\n",
    "{{\n",
    "  \"entree\": [ ... ],\n",
    "  \"plats principaux\": [ ... ],\n",
    "  \"desserts\": [ ... ]\n",
    "}}\n",
    "en changeant les clés selon les catégories de plats.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voici un menu de restaurant sous forme de texte non structuré :\n",
      "\n",
      "ENTRÉE\n",
      "BUFALA E RUCOLA\n",
      "tomates-cerise, mozzarella, huile d'olive extra vierge, basilic & roquette.\n",
      "\n",
      "PLAT PRINCIPAL\n",
      "SAUMON GRILLÉ\n",
      "saumon bio grillé servi avec scarole sautée.\n",
      "\n",
      "TAGLIATA\n",
      "piccata de veau au citron & fleurs de câpres.\n",
      "\n",
      "DESSERT\n",
      "TIRAMISÜ\n",
      "tiramisü classique aux copeaux de chocolat avec crème fouettée.\n",
      "\n",
      "\n",
      "Structure le texte en JSON sous la forme suivante :\n",
      "{\n",
      "  \"entree\": [ ... ],\n",
      "  \"plats principaux\": [ ... ],\n",
      "  \"desserts\": [ ... ]\n",
      "}\n",
      "en changeant les clés selon les catégories de plats.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Génération de texte\n",
    "result = llama_pipeline(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Résultat généré\n",
    "generated_text = result[0]['generated_text']\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaume/Documents/Hackathon/SmartMenue/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:2869: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0730954789043e8852b764f7ff3d310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./models/llama_model/tokenizer_config.json',\n",
       " './models/llama_model/special_tokens_map.json',\n",
       " './models/llama_model/tokenizer.model',\n",
       " './models/llama_model/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chemin où vous voulez enregistrer le modèle\n",
    "save_directory = \"./models/llama_model\"\n",
    "\n",
    "# Enregistrer le modèle\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Enregistrer le tokenizer\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec473722e24046ec9a9372ffea3b8ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('../models/flan_model_xl/tokenizer_config.json',\n",
       " '../models/flan_model_xl/special_tokens_map.json',\n",
       " '../models/flan_model_xl/spiece.model',\n",
       " '../models/flan_model_xl/added_tokens.json',\n",
       " '../models/flan_model_xl/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "save_directory=\"../models/flan_model_xl\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaume/Documents/Hackathon/SmartMenue/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The student is learning computer science. He is learning how to program. He is learning how to']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Write a story about a student who is learning computer science\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> entr<unk>e BUFALA E RUCOLA tomates-cerise,\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ENTRÉE\\n\\nBUFALA E RUCOLA\\n\\ntomates-cerise, mozzarella, huile d'olive\\nextra vierge, basilic & roquette.\\n\\nPLAT PRINCIPAL\\n\\nSAUMON GRILLÉ\\n\\nsaumon bio grillé servi avec scarole\\nsautée\\n\\nTAGLIATA\\n\\npiccata de veau au citron & fleurs de\\ncâpres\\n\\nDESSERT\\n\\nTIRAMISÜ\\n\\ntiramisü classique aux copeaux de\\nchocolat avec crème fouettée\\n\\n generate a json file from this text with categories and items\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donne moi dictionnaire Python structuré avec le nom des catégories de plats comme clés et les plats correspondants comme valeurs.\n"
     ]
    }
   ],
   "source": [
    "# Texte brut extrait par PyTesseract\n",
    "input_text = \"\"\"\n",
    "+\\n\\nENTRÉE\\n\\nBUFALA E RUCOLA\\n\\ntomates-cerise, mozzarella, huile d'olive\\nextra vierge, basilic & roquette.\\n\\nPLAT PRINCIPAL\\n\\nSAUMON GRILLÉ\\n\\nsaumon bio grillé servi avec scarole\\nsautée\\n\\nTAGLIATA\\n\\npiccata de veau au citron & fleurs de\\ncâpres\\n\\nDESSERT\\n\\nTIRAMISÜ\\n\\ntiramisü classique aux copeaux de\\nchocolat avec crème fouettée\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Voici un menu de restaurant extrait sous forme de texte brut :\n",
    "{input_text}\n",
    "\n",
    "Donne moi dictionnaire Python structuré avec le nom des catégories de plats comme clés et les plats correspondants comme valeurs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Encoder l'input long\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "# Générer la sortie\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=1000,  # Ajuster pour la taille de la sortie attendue\n",
    "    num_beams=4,     # Améliorer la précision de génération\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# Décoder et afficher le résultat\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137db5f554f44b7a922ce943f92e0ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719d98b23c5c49cf8831ff3acf1755b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a4bb08d9034e4abf30c498819380d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8eeda072a548d0b6159f41ce7b97d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa92583f478a4f10a9e606d5c73df08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bc17694819475d8a7f754a0f3e38c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952f0e5b41194a26a4ea26d261231494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "# Charger le modèle Flan-T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a restaurant menu: Entrées: - Salade verte - Soupe du jour - Foie gras maison Plats: - Poulet rôti - Saumon grillé - Filet de buf Desserts: - Tarte au citron - Fondant au chocolat - Glaces artisanales Boissons: - Eau minérale - Soda - Café Vins: - Bordeaux rouge - Chardonnay blanc Transforme this text into a strictly structured JSON. Each section (Entrées, Plats, Desserts, Boissons, Vins) must be a clé. The relevant elements doivent be list. There is nothing else than the JSON.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "menu_text = \"\"\"\n",
    "Entrées:\n",
    "- Salade verte\n",
    "- Soupe du jour\n",
    "- Foie gras maison\n",
    "\n",
    "Plats:\n",
    "- Poulet rôti\n",
    "- Saumon grillé\n",
    "- Filet de bœuf\n",
    "\n",
    "Desserts:\n",
    "- Tarte au citron\n",
    "- Fondant au chocolat\n",
    "- Glaces artisanales\n",
    "\n",
    "Boissons:\n",
    "- Eau minérale\n",
    "- Soda\n",
    "- Café\n",
    "\n",
    "Vins:\n",
    "- Bordeaux rouge\n",
    "- Chardonnay blanc\n",
    "\"\"\"\n",
    "prompt = (\n",
    "    f\"Voici un menu de restaurant :\\n\\n{menu_text}\\n\\n\"\n",
    "    \"Transforme ce texte en un JSON strictement structuré. Chaque section (Entrées, Plats, Desserts, Boissons, Vins) \"\n",
    "    \"doit être une clé. Les éléments correspondants doivent être des listes. Ne réponds rien d'autre que le JSON.\"\n",
    ")\n",
    "\n",
    "# Tokenizer le prompt et générer le texte de sortie\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(**inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Décoder la sortie en texte\n",
    "result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Tokenizer le prompt et générer le texte de sortie\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(**inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Décoder la sortie en texte\n",
    "result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "result_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
